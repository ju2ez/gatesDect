apache spark is one of the most popular technologies on the big data landscape as a framework for distributed computing it allows users to scale to massive datasets by running computations in parallel either on a single machine or on clusters of thousands of machines link can be used with scala r java sql or python code and link have led to a rapid adoption as the size of datasets and the need for methods to work with them increase after using link with link by running calculations in parallel on a single multi core machine we wanted to see if we could use a similar approach with spark to scale to a cluster of multiple machines while dask can also be used for cluster computing we wanted to demonstrate that featuretools can run on multiple distributed computing frameworks the same feature engineering code that runs in parallel using dask requires no modification to also be distributed with spark in this article we ll see how to use spark with link to run link on a computing cluster to scale to even larger datasets the code for this article is available as a link 