there is a short answer to this question so let s start there then we can take a look at an application to get a little more insight a link is a grid of link link say numbers surrounded by brackets we can add and subtract matrices of the same size multiply one matrix with another as long as the sizes are compatible link link link link link link and multiply an entire matrix by a constant a vector is a matrix with just one row or column but see below so there are a bunch of mathematical operations that we can do to any matrix the basic idea though is that a matrix is just a d grid of numbers a link is often thought of as a generalized matrix that is it could be a d matrix a vector is actually such a tensor a d matrix something like a cube of numbers even a d matrix a single number or a higher dimensional structure that is harder to visualize the dimension of the tensor is called its link but this description misses the most important property of a tensor a tensor is a mathematical entity that lives in a structure and interacts with other mathematical entities if one link the other entities in the structure in a regular way then the tensor link this dynamical property of a tensor is the key that distinguishes it from a mere matrix it s a team player whose numerical values shift around along with those of its teammates when a transformation is introduced that affects all of them any rank tensor can be represented as a matrix but not every matrix is really a rank tensor the numerical values of a tensor s matrix representation depend on what transformation rules have been applied to the entire system this answer might be enough for your purposes but we can do a little example to illustrate how this works the question came up in a deep learning workshop so let s look at a quick example from that field suppose i have a hidden layer of nodes in a neural network data flowed into them went through their relu functions and out popped some values let s say for definiteness we got and respectively don t worry a diagram is coming we could represent these nodes output as a vector let s say there s another layer of nodes coming up each of the nodes from the first layer has a weight associated with its input to each of the next nodes it would be very convenient then to write these weights as a matrix of entries suppose we ve updated the network already many times and arrived at the weights chosen semi randomly for this example here the weights from one row all link the same node in the next layer and those in a particular column all link the same node in the first layer for example the weight that incoming node contributes to outgoing node is row col we can compute the total values fed into the next layer of nodes by multiplying the weight matrix by the input vector don t like matrices here s a diagram the data flow from left to right great so far all we have seen are some simple manipulations of matrices and vectors but suppose i want to meddle around and use custom activation functions for each neuron a dumb way to do this would be to rescale each of the relu functions from the first layer individually for the sake of this example let s suppose i scale the first node up by a factor of leave the second node alone and scale the third node down by this would change the graphs of these functions as pictured below the effect of this modification is to change the values spit out by the first layer by factors of and respectively that s equivalent to multiplying l by a matrix a now if these new values are fed through the original network of weights we get totally different output values as illustrated in the diagram if the neural network were functioning properly before we ve broken it now we ll have to rerun the training to get the correct weights back or will we the value at the first node is twice as big as before if we cut all of its outgoing weights by its net contribution to the next layer is unchanged we didn t do anything to the second node so we can leave its weights alone lastly we ll need to multiply the final set of weights by to compensate for the factor on that node this is equivalent mathematically speaking to using a new set of weights which we obtain by multiplying the original weight matrix by the inverse matrix of a if we combine the modified output of the first layer with the modified weights we end up with the correct values reaching the second layer hurray the network is working again despite our best efforts to the contrary ok there s been a ton of math so let s just sit back for a second and recap when we thought of the node inputs outputs and weights as fixed quantities we called them vectors and matrices and were done with it but once we started monkeying around with one of the vectors link it in a regular way we had to compensate by link the weights in an opposite manner this added integrated structure elevates the mere matrix of numbers to a true link object in fact we can characterize its tensor nature a little bit further if we call the changes made to the nodes link ie varying link the node and multiplied by a that makes the weights a link tensor varying link the nodes specifically multiplied by the inverse of a instead of a itself a tensor can be covariant in one dimension and contravariant in another but that s a tale for another day and now you know the difference between a matrix and a tensor 